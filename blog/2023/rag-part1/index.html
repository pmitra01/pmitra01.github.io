<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Biomedical RAG on your local machine - ft. LLaMA + Qdrant | Payal  Mitra</title>
    <meta name="author" content="Payal  Mitra">
    <meta name="description" content="Implementing RAG pipeline for scientific conversational QA on your local machine">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://pmitra01.github.io/blog/2023/rag-part1/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Biomedical RAG on your local machine - ft. LLaMA + Qdrant",
      "description": "Implementing RAG pipeline for scientific conversational QA on your local machine",
      "published": "November 23, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Payal </span>Mitra</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Biomedical RAG on your local machine - ft. LLaMA + Qdrant</h1>
        <p>Implementing RAG pipeline for scientific conversational QA on your local machine</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h2 id="tldr">TL;DR</h2>

<p>Retrieval Augmented Generation (RAG) [1,2,3] is a popular LLM-application pattern to incorporate external domain 
knowledge from retrieval databases to condition the generative LLM output. This post implements a RAG pipeline with 
Llama 2 on one’s local machine for long-form QA on medical text content. Three reasons for this post:</p>
<ol>
  <li>
    <p>This is Part 1 of the 2-part blog series. It lays down the necessary RAG pipeline apparatus and baseline
performance for the task of long-form medical QA that will be required later in Part 2 where I experiment with
fine-tuning the LLM.</p>
  </li>
  <li>
    <p>For democratised access, I wanted to implement RAG entirely on my local machine with smaller LLMs. No expensive Google Colab compute or LLMs 
behind remote APIs.</p>
  </li>
  <li>
    <p>instead of viewing all the concepts in isolation, I wanted to integrate the many exciting ideas (LLM 
quantisation, Qdrant, fastembed, llama.cpp, etc) within a single project.</p>
  </li>
</ol>

<p><strong>Why you may want to read this</strong>: 
You are interested in conversational knowledge applications such as long-form QA, chatbots or search in 
specialised domains (e.g. medicine/finance). OR you want to compare implementation notes for RAG, vector search, 
etc. Or, you may want to build generative LLM applications locally with small models and are interested in enabling 
tools such as llama.cpp and LLM quantisation.</p>

<h2 id="deep-dive">Deep Dive</h2>
<p><strong>Motivation:</strong> To build a conversational interface for a knowledge-retrieval application (be it search, question 
answering, or chat agents) RAG has become the standard pattern for integrating external domain knowledge into generative LLM 
workflows. I like to think of this task as an open book exam, where the book chapters are selected by the retrieval mechanism and the LLM 
is prompted for reading comprehension on this retrieved context. It goes a long way in overcoming limitations of foundation LLMs (hallucinations, being 
trained on static outdated data, lack of domain knowledge, etc) by augmenting the LLM’s intrinsic parametric memory 
with external vector databases/retrieval mechanisms.</p>

<p>Note, RAG workflows commonly adapt off-the-shelf LLMs using sophisticated prompt-engineering techniques
to coax the LLM to perform a desired generative task, rather than fine-tuning the model.
Relying solely on <a href="https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610" rel="external nofollow noopener" target="_blank">In-Context-Learning</a><d-footnote> 
A more advanced read on topic can be found in Stanford AI Blog's 'How in-context-learning works': 
https://ai.stanford.edu/blog/understanding-incontext/ </d-footnote> from prompts at inference time to perform 
complex reasoning tasks is brittle and has implications on 
correctness of LLM answer, consistency of output format, high inference costs due to lengthy multi-step prompts, etc.
The challenges are amplified when operating in domains that have very stringent safety concerns and near zero 
tolerance for misinformation. Fine-tuning is known to produce more reliable and consistent LLM output than solely 
relying on in-context learning through clever prompts. That experiment is left to Part 2 of this series.</p>

<h3 id="task-details">Task Details</h3>
<p>The premise of this tutorial is applying generative LLMs within a RAG framework for a (1) conversational QA task, 
focussed on (2) niche domains which are expected to require more specialised knowledge than the general purpose 
training corpora used for the foundation LLMs.</p>

<p>We select the task of long-form question answering based on biomedical research content from PubMed journal articles.</p>
<ul>
  <li>
    <p>Task: <a href="https://pubmedqa.github.io/" rel="external nofollow noopener" target="_blank">PubMedQA</a></p>

    <p>The official website describes this as <code class="language-plaintext highlighter-rouge">to answer research questions with yes/no/maybe using the corresponding 
  abstracts.</code> The dataset also includes long form answers in addition to the closed form answers of yes/no/maybe. 
  We will focus on the task of long-form question answering.</p>
  </li>
  <li>
    <p>Dataset (converted into instruction tuning format): <a href="https://huggingface.co/datasets/FedML/PubMedQA_instruction" rel="external nofollow noopener" target="_blank">Huggingface datasets hub</a></p>

    <p>Here is a snippet of the instruction tuning dataset. Note, during instruction-tuning, the model would be trained on all 
three components of a data point - instruction question, context and answer. However during model inference, as is 
the focus of this blog, we will only use the instruction question, fetch the retrieved context from the vector 
search pipeline, and prompt the LLM to generate the final answer based on this context.</p>
  </li>
</ul>

<div class="row mt-3" style="width:1000px; height:600px align:center">
    <div class="col-sm mt-3 mt-md-0">
    <figcaption> Figure 1: Sample from PubMedQA dataset converted for instruction-tuning
</figcaption>
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rag_llama_1/pubmedqa_instruction_sample-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rag_llama_1/pubmedqa_instruction_sample-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rag_llama_1/pubmedqa_instruction_sample-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rag_llama_1/pubmedqa_instruction_sample.png" class="img-fluid 
rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<h2 id="implementation">Implementation</h2>

<p>Here is an overview of the steps in the implementation workflow, in sequence</p>
<ul>
  <li>Prerequisites
    <ol>
      <li><a href="#prereq-llamacpp">Download LLaMA model, and prepare for inference locally</a></li>
      <li><a href="#prereq-qdrant">Vectorise text documents in a vector database, and create searchable index</a></li>
    </ol>
  </li>
  <li>RAG inference workflow for each incoming user query:
    <ol>
      <li><a href="#rag-retrieval">retrieve relevant documents by querying vector database (here, Qdrant search engine)</a></li>
      <li><a href="#rag-promptconstruct">construct an LLM prompt with user question, and relevant documents - instructing LLM to answer from within the
 provided context</a></li>
      <li><a href="#rag-promptllm">Send prompt to LLM server to generate an answer</a></li>
      <li>Optional - Postprocess LLM output (parsing, add guardrails) before displaying to user</li>
    </ol>
  </li>
</ul>

<p>Let’s elaborate on each of these steps</p>

<h3 id="implementation---prerequisites">Implementation - Prerequisites</h3>
<ol>
  <li>
<a name="prereq-llamacpp">Prepare LLaMA model to deploy for inference locally, using llama.cpp project</a>
This project enables running LLaMA 2 model variants in quantized form for inference on one’s local regular 
purpose machine. The llama.cpp can be deployed as a cli tool, or deploy it as a server (See example
<a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#quick-start" rel="external nofollow noopener" target="_blank">here</a>)
    <ul>
      <li>Download Llama model (weights, tokenizer, etc) using the meta request form and instructions provided <a href="https://github.com/facebookresearch/llama?fbclid=IwAR3pMaYrinZM5-DEp8i1IaLtTxKe7lNbEzv_-b8_9DMKOGXvRxuqSRZYEgs#download" rel="external nofollow noopener" target="_blank">here</a>
</li>
      <li>Install the llama.cpp project: Follow comprehensive setup instructions provided <a href="https://github.com/ggerganov/llama.cpp#usage" rel="external nofollow noopener" target="_blank">here</a>
</li>
      <li>Quantise the desired llama model (Following instructions from llama.cpp as seen <a href="https://github.com/ggerganov/llama.cpp#prepare-data--run" rel="external nofollow noopener" target="_blank">here</a>). I used the 7B and 7B-chat models. In a nutshell:</li>
    </ul>
    <ul>
      <li>After obtaining the original LLaMA weights, place them in ./models</li>
      <li>Install python dependencies from requirements.txt file</li>
      <li>Convert the model to <a href="https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GGUF#about-gguf" rel="external nofollow noopener" target="_blank">GGUF</a> FP16 format <code class="language-plaintext highlighter-rouge">python3 convert.py models/7B-chat/</code>
</li>
      <li>Perform model quantization. This is a technique to reduce the memory footprint of the LLM weights by using a 
different data type to store the entries of the weight matrices. Read more about it <a href="https://huggingface.co/blog/hf-bitsandbytes-integration" rel="external nofollow noopener" target="_blank">here</a>
Below I perform 8-bit model quantization of the ggml model
        <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>   <span class="p">.</span><span class="o">/</span><span class="n">quantize</span> <span class="p">.</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="mi">7</span><span class="n">B</span><span class="o">-</span><span class="n">chat</span><span class="o">/</span><span class="n">ggml</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">f16</span><span class="p">.</span><span class="n">gguf</span> <span class="p">.</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="mi">7</span><span class="n">B</span><span class="o">-</span><span class="n">chat</span><span class="o">/</span><span class="n">ggml</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">q8_0</span><span class="p">.</span><span class="n">gguf</span> <span class="n">q8_0</span>
</code></pre></div>        </div>
        <p>The quantized model thus obtained will be used under the hood for the RAG pipeline below</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a name="prereq-qdrant">Vectorise text documents in a vector database, and create searchable index</a></p>

    <p>a. Obtain PubMedQA_instruction from <a href="https://huggingface.%0A%20%20%20co/datasets/FedML/PubMedQA_instruction" rel="external nofollow noopener" target="_blank">Huggingface datasets hub</a>. This is pre-formats the PubMedQA dataset for instruction-tuning.</p>
    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code>    <span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
    <span class="n">dataset_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">FedML/PubMedQA_instruction</span><span class="sh">"</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">dataset_name</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train[0:2000]</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>b. Vectorise data using <code class="language-plaintext highlighter-rouge">qdrant-fastembed</code> and create a searchable vector index in <a href="https://qdrant.tech/" rel="external nofollow noopener" target="_blank">Qdrant</a></p>

    <ul>
      <li>Install Qdrant’s <code class="language-plaintext highlighter-rouge">FastEmbed</code> - ‘a lightweight, fast, Python library built for embedding generation’, 
following instructions <a href="https://github.com/qdrant/fastembed#usage-with-qdrant" rel="external nofollow noopener" target="_blank">here</a>.</li>
      <li>Install qdrant-client, a python client to interface with the Qdrant vector database and search engine following 
instructions <a href="https://github.com/qdrant/qdrant-client#fast-embeddings--simpler-api" rel="external nofollow noopener" target="_blank">here</a>.</li>
      <li>Qdrant allows the developer to customise the search index config - you can annotate a single 
 document with multiple vectors, or with additional payload (e.g. dates, titles, full text bodies, etc) that can be 
 used as document filters or key-word searches to augment vector search</li>
    </ul>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="n">qdrant_client</span> <span class="o">=</span> <span class="nc">QdrantClient</span><span class="p">(</span><span class="sh">"</span><span class="s">:memory:</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">EMBEDDING_MODEL_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">BAAI/bge-base-en</span><span class="sh">"</span>
 <span class="n">qdrant_client</span><span class="p">.</span><span class="nf">set_model</span><span class="p">(</span><span class="n">EMBEDDING_MODEL_NAME</span><span class="p">)</span>
 <span class="n">COLLECTION_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">pubmedqa</span><span class="sh">"</span>
    
 <span class="n">qdrant_client</span><span class="p">.</span><span class="nf">create_collection</span><span class="p">(</span>
     <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
     <span class="n">vectors_config</span><span class="o">=</span><span class="n">qdrant_client</span><span class="p">.</span><span class="nf">get_fastembed_vector_params</span><span class="p">()</span>
 <span class="p">)</span>
    
 <span class="n">qdrant_client</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span>
     <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
     <span class="n">documents</span><span class="o">=</span><span class="p">[</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">data</span><span class="p">],</span>
     <span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">doc</span><span class="p">[</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
 <span class="p">)</span>
    
 <span class="c1"># Qdrant search engine client makes it very easy to search a query against a database, and retrieve top-k documents 
</span>    
 <span class="n">search_hits</span> <span class="o">=</span> <span class="n">qdrant_client</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span>
     <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
     <span class="n">query_text</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
     <span class="n">limit</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>

</code></pre></div>    </div>
  </li>
</ol>

<h3 id="implementation---rag-inference-workflow-on-each-user-query">Implementation - RAG inference workflow on each user query</h3>
<ol>
  <li>
    <p><a name="rag-retrieval">Retrieve relevant documents by querying the vector DB using Qdrant’s semantic search</a> 
engine API</p>

    <p>Qdrant search engine client makes it very easy to search a query against a database, and retrieve top-k documents 
Let’s write a simple function that the RAG pipeline can use to retrieve relevant context from the Qdrant vector 
database at inference time.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="c1"># define class retriever/ vectorstore_as_retriever
</span> <span class="k">def</span> <span class="nf">retrieve_relevant_doc_context</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">qdrant_client</span><span class="p">,</span> <span class="n">collection_name</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
     <span class="n">rel_docs</span> <span class="o">=</span> <span class="p">[]</span>
     <span class="n">search_hits</span> <span class="o">=</span> <span class="n">qdrant_client</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span>
         <span class="n">collection_name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>
         <span class="n">query_text</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
         <span class="n">limit</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">hit</span> <span class="ow">in</span> <span class="n">search_hits</span><span class="p">:</span>
         <span class="n">hit_dict</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">hit</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="sh">'</span><span class="s">document</span><span class="sh">'</span><span class="p">],</span>
                     <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">:</span> <span class="n">hit</span><span class="p">.</span><span class="n">score</span><span class="p">}</span>
         <span class="n">rel_docs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">hit_dict</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">])</span>
         <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
             <span class="nf">print</span><span class="p">(</span><span class="n">hit_dict</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">rel_docs</span>

 <span class="n">rel_docs</span> <span class="o">=</span> <span class="nf">retrieve_relevant_doc_context</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
                              <span class="n">qdrant_client</span><span class="o">=</span><span class="n">qdrant_client</span><span class="p">,</span>
                              <span class="n">collection_name</span><span class="o">=</span><span class="n">COLLECTION_NAME</span><span class="p">,</span>
                              <span class="n">top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

</code></pre></div>    </div>
  </li>
  <li>
    <p><a name="#rag-promptconstruct">Construct an LLM prompt with user question, and relevant documents - instructing 
LLM to answer from within the provided context </a></p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
    
 <span class="n">RAG_PROMPT_string</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"""</span><span class="se">\
</span><span class="s"> Human: Here is a question from a medical professional: 
&lt;question&gt; 
{user_query}
&lt;/question&gt;
    
 Here are some search results from a medical encyclopedia that you must reference to answer the question: 
 {extracts}
    
 Once again, here is the question:
 &lt;question&gt;
 {user_query}
 &lt;/question&gt;
    
 Your objective is to write a high quality, concise answer
 for the medical professional within &lt;answer&gt; &lt;/answer&gt; tags. Otherwise, write ANSWER NOT FOUND)
    
 Assistant: &lt;answer&gt;</span><span class="se">\n\n</span><span class="s"> </span><span class="sh">"""</span> 
 <span class="p">)</span>
    
 <span class="n">rag_prompt_template</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">RAG_PROMPT_string</span><span class="p">)</span>
</code></pre></div>    </div>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">List</span>
 <span class="k">def</span> <span class="nf">prep_rag_prompt</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                     <span class="n">rel_search_extracts</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
                     <span class="n">prompt_template</span><span class="p">,</span>
                    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
     <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">extracts</span><span class="o">=</span><span class="sh">'</span><span class="se">\n\n</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">rel_docs</span><span class="p">),</span>
                                          <span class="n">user_query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
                                          <span class="p">)</span>
     <span class="k">return</span> <span class="n">prompt</span>
 <span class="n">rag_prompt</span> <span class="o">=</span> <span class="nf">prep_rag_prompt</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">rel_search_extracts</span> <span class="o">=</span> <span class="n">rel_docs</span><span class="p">,</span> <span class="n">prompt_template</span><span class="o">=</span><span class="n">rag_prompt_template</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p><a name="#rag-promptllm">Prompting the LLaMA model within the RAG pipeline.</a> 
Calling the local quantized LLaMA model from behind the llama.cpp app is made very easy by the llama-cpp-python 
project. This provides python bindings to the llama.cpp project.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">LlamaCpp</span>
 <span class="kn">from</span> <span class="n">langchain.callbacks.manager</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
 <span class="kn">from</span> <span class="n">langchain.callbacks.streaming_stdout</span> <span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>
    
 <span class="n">n_gpu_layers</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Metal set to 1 is enough.
</span> <span class="n">n_batch</span> <span class="o">=</span> <span class="mi">512</span>  <span class="c1"># Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.
</span> <span class="n">callback_manager</span> <span class="o">=</span> <span class="nc">CallbackManager</span><span class="p">([</span><span class="nc">StreamingStdOutCallbackHandler</span><span class="p">()])</span>
 <span class="n">LLAMA_CPP_Q8_PATH</span> <span class="o">=</span> <span class="sh">"</span><span class="s">/Users/mitrap/PycharmProjects/llama.cpp/models/7B-chat/ggml-model-q8_0.gguf</span><span class="sh">"</span>
    
 <span class="c1"># Make sure the model path is correct for your system!
</span> <span class="n">llm</span> <span class="o">=</span> <span class="nc">LlamaCpp</span><span class="p">(</span>
     <span class="n">model_path</span><span class="o">=</span><span class="n">LLAMA_CPP_Q8_PATH</span><span class="p">,</span>
     <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">n_gpu_layers</span><span class="p">,</span>
     <span class="n">n_batch</span><span class="o">=</span><span class="n">n_batch</span><span class="p">,</span>
     <span class="n">n_ctx</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
     <span class="n">f16_kv</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># MUST set to True, otherwise you will run into problem after a couple of calls
</span>     <span class="n">callback_manager</span><span class="o">=</span><span class="n">callback_manager</span><span class="p">,</span>
     <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
 <span class="p">)</span>
</code></pre></div>    </div>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code> <span class="n">rag_prompt</span> <span class="o">=</span> <span class="nf">prep_rag_prompt</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
                              <span class="n">rel_search_extracts</span> <span class="o">=</span> <span class="n">rel_docs</span><span class="p">,</span>
                              <span class="n">prompt_template</span><span class="o">=</span><span class="n">rag_prompt_template</span><span class="p">)</span>
    
 <span class="nf">llm</span><span class="p">(</span><span class="n">rag_prompt</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>The code illustrating the above workflow is made available on <a href="https://github.com/pmitra01/llm_peft_exploration/tree/main/rag_with_llama2" rel="external nofollow noopener" target="_blank">github</a></p>

<p>References:</p>
<ol>
  <li>IBM - What is Retrieval Augmented Generation? <a href="https://research.ibm.%0Acom/blog/retrieval-augmented-generation-RAG" rel="external nofollow noopener" target="_blank">[Link]</a>
</li>
  <li>Prompt Engineering Guide - Retrieval Augmented Generation (RAG) <a href="https://www.promptingguide.ai/techniques/rag" rel="external nofollow noopener" target="_blank">[Link]</a>
</li>
  <li>The Complete Overview to Retrieval Augmented Generation (RAG) <a href="https://medium.%0Acom/gopenai/the-complete-guide-to-retrieval-augmented-generation-rag-3ce54a57d8be" rel="external nofollow noopener" target="_blank">[Link]</a>
</li>
  <li>In-Context-Learning</li>
  <li>FastEmbed: Fast and Lightweight Embedding Generation for Text <a href="https://qdrant.tech/articles/fastembed/" rel="external nofollow noopener" target="_blank">[Link]</a>
</li>
</ol>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Payal  Mitra. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
