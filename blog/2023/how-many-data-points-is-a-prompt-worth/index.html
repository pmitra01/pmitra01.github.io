<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>How many data points is a prompt worth? | Payal  Mitra</title>
    <meta name="author" content="Payal  Mitra">
    <meta name="description" content="">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://pmitra01.github.io/blog/2023/how-many-data-points-is-a-prompt-worth/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "How many data points is a prompt worth?",
      "description": "",
      "published": "October 16, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Payal </span>Mitra</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>How many data points is a prompt worth?</h1>
        <p></p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <h2 id="tldr">TL;DR</h2>
<p>This is a simple post that journals two things. 
First, the long urge I have had to conduct a basic experiment to see for myself ‘how many (training) data points a 
(training) prompt was worth’. Given a task we wish to train an LLM for, can we yield comparable or even better 
models with a mere fraction of the labeled data points than before, by simply transforming the training examples 
differently? I share the experiment setup, code and observations from fine-tuning a series of models on the 
rudimentary task of topic classification which illustrates the sample efficiency of instruction tuning with prompts.</p>

<p>Second, it consolidates reflections on what makes prompting powerful: (1) tracing how the use of prompting an LLM 
for completion is at the heart of today’s class of LLM fine-tuning methods (instruction tuning, prefix tuning, RLHF, 
etc). It makes the fine-tuning objective more consistent with the pretraining objective; (2) how non-generative 
tasks can be creatively reformulated as generative tasks (3) when to consider fine-tuning over prompt engineering</p>

<p>Note: I’m not making a case for ‘LLM-maximalism’ where the perfect prompt and best-on-market LLM combo magically 
solves compound NLP tasks. On the contrary to build reliable and feasible production-grade systems with the 
relatively untamed LLMs, I identify with: (i) <a href="https://explosion.ai/blog/against-llm-maximalism" rel="external nofollow noopener" target="_blank">LLM-Pragmatism</a> and 
<a href="https://eugeneyan.com/writing/llm-patterns/?utm_source=convertkit&amp;utm_medium=email&amp;utm_campaign=How+to+Match+LLM+Patterns+to+Problems%20-%2011495339#evals-to-measure-performance" rel="external nofollow noopener" target="_blank">Evaluation-Driven-Development</a> [2] 
(EDD?). Depending on your use-case LLM Pragmatism could call for some combination of task-composibility and 
control-flows that coordinate different components and tools. <d-footnote>Instead of aggregating all steps to be 
performed by the product feature into one big LLM prompt, decompose it into subtasks and delegate different 
components to heuristic or more explainable ML solutions where best suited, and LLM-generation only for certain 
aspects such as summarisation or intent interpretation, which are highly abstractive in nature. The decision flow of 
control between different components can be determined with agents or rules. Examples include RAG (Retrieval 
Augmented Generation) in search, or agent-based LLM workflows. Several other examples can also be found in [Building 
LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html) [1] and 
[Patterns for Building LLM-Based systems and products](https://eugeneyan.com/writing/llm-patterns/) [2].
</d-footnote>. (ii) It is important to understand when and how to fine-tune a suitable (possibly ‘small’) model for 
one’s use case.</p>

<h2 id="diving-in">Diving in</h2>
<p>The genesis of this blog post was in early November 2022 after attending an <a href="https://www.akbc.ws/2022/" rel="external nofollow noopener" target="_blank">AKBC</a> 
workshop. During two of the keynote talks <d-footnote>[Keynote talks](https://wise-supervision.github.io/#Speakers)
by Prof. Heng Ji introducing [Code4Struct](https://arxiv.org/abs/2210.12810); and Prof. Eneko Agirre on ['Pretrain, 
Prompt, Entail'](https://dl.acm.org/doi/abs/10.1145/3477495.3532786) paradigm for information extraction.
</d-footnote>, it first dawned upon me how simple, versatile and effective the tactic of prompting was not just for 
in-context learning at inference time, but also to further fine-tune LLMs on desired tasks. Prompts allowed the 
framing of tasks in a way that was closer to the pretraining objective (next token/sentence prediction) of a class 
of LLMs. Suddenly I felt as though I had been looking at so many of the NLP tasks around knowledge extraction 
unhelpfully for years. Made me think about some problems from first principles again.</p>

<p>But then the ChatGPT-era happened and this newfound wonder with prompts felt antiquated, followed by personal 
genAI-fatigue. However, I am ready to revive this article because (1) I had to scratch the itch, (2) the intuition 
of prompt guided learning is still fundamental in (almost all?) approaches to fine-tuning LLMs and can often feel 
hidden beneath the abstractions of handy libraries and APIs around generative models.</p>

<p>The structure of the post is as follows:</p>
<ol>
  <li><a href="#1-experiment">Experiment on sample-efficiency of prompt-based fine-tuning Vs. conventional methods</a></li>
  <li><a href="#2-reflections">Reflections on fine-tuning LLMs with prompts</a></li>
  <li><a href="#3-takeaway-remarks">Observatory Note on use of prompt engineering vs fine-tuning</a></li>
</ol>

<h3 id="1-experiment">Experiment</h3>
<p>To quantitatively observe the sample efficiency of fine-tuning LLMs using prompt-completion pairs instead of 
conventional supervised  classification on input-output labelled data points, I performed the following experiment 
inspired by the paper ‘<a href="(https://arxiv.org/abs/2103.08493)">How many data points is a prompt worth</a>’ [6]. The 
authors perform a number of experiments on 6 different benchmark NLP tasks and find that prompt-based instruction 
tuned LLMs outperform the classifier-head based model, while needing 100x-1000x fewer prompt data points. In my 
version of the experiment, I kept things lightweight with a rudimentary causal LM GPT2, and a simple discriminative task.</p>

<p><strong>Experiment Setup</strong></p>
<ul>
  <li>
<strong>Task</strong>: Topic classification of news articles</li>
  <li>
<strong>Dataset</strong>: ag-news (<a href="https://huggingface.co/datasets/ag_news" rel="external nofollow noopener" target="_blank">huggingface dataset hub</a>)</li>
  <li>
    <p><strong>Pretrained LLM</strong>: GPT-2 (for a <code class="language-plaintext highlighter-rouge">$$</code>-and-resource friendly experiment)</p>
  </li>
  <li>Comment on choice of task and dataset:
    <ul>
      <li>I was inclined towards testing generative LLMs on discriminative NLP tasks because discriminative tasks are more 
easily verifiable for accuracy than abstractive or open-ended language tasks. <d-footnote>Note on roads not taken: 
There is no reason why this experiment could not be done on harder sequence extractive/classification tasks such 
as extractive QA, or generative abstractive tasks. While those would have made for a more satisfying experiment, 
I abandoned my training efforts mid-way, in favour of cheaper topic classification. Purely generative tasks which do not 
have deterministic outputs (e.g. abstractive summarisation or Q/A) were left out of scope as they were 
harder to evaluate objectively.</d-footnote>
</li>
      <li>Discriminative vs generative setup: The experiment trains a series of models on text classification task in two 
training paradigms using the same base LLM. The former training setup uses the more conventional approach of 
fitting a classifier head after LLM layers, and training the classifier (with or without frozen LLM layers) to 
map input documents to a fixed set of output labels. The latter setup directly tunes (all or part of) the LLM 
layers to autoregressively generate the correct output label given a prompt containing an instruction and the 
input document text to be classified. Each model is trained only for 5 epochs.</li>
      <li>Label Predictions: To obtain a class label for input sentences in the generative case, I used a verbaliser to 
map the output of the first 5 generated tokens from the fine-tuned LLM to map to one of the 4 class labels using 
a scoring function such as bert-score and rouge metrics.</li>
      <li>Varying the dataset size for training: I implement a custom sequential data sampler in Pytorch that increments 
the training data volume exponentially to the base 10. I start my training runs with 10 data points, and then 
sequentially increment the training set to 100, 1000, 10000, 100000 data points. In the reference paper, the 
authors use a similar exponential philosophy. Additionally, they perform multiple data sampling runs to build 
more robust training curves (I skip that aspect).</li>
      <li>Training curves: I then obtain training curves for each setup, by training 5 different models on incremental 
segments of the labeled dataset. The classification accuracy on a held out test set is plotted for each 
model tuned on varying training data sizes.</li>
      <li>Verdict: The hold-out test-set performance trends for both classifier head and the prompt-based instruction tuning 
scenarios are compared to compute the relative data advantage either method may have on the other.</li>
    </ul>
  </li>
</ul>

<h4 id="implementation-notes">Implementation notes</h4>

<ul>
  <li>I implement a custom sequential data sampler in Pytorch that increments the training data volume exponentially to the 
base 10. I start my training runs with 10 data points, and then sequentially increment the training set to $100$, 
1000, 10000, 100000 data points. In the reference paper, the authors use a similar exponential philosophy. 
Additionally, they perform multiple data sampling runs to build more robust training curves. I slack off on this aspect.</li>
  <li>Link to code: TBD</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">Setup 1</th>
      <th style="text-align: center">Setup 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Input</td>
      <td style="text-align: center">Text of news articles. <br> Example.: “<em>South Korea lowers interest rates South Korea’s central bank cuts interest rates by a quarter percentage point to 3.5 in a bid to drive growth in the economy.</em>”</td>
      <td style="text-align: center">Text of news article embedded in a prompt template. Example: “<code class="language-plaintext highlighter-rouge">Given news article:</code> <em>South Korea lowers interest rates South Korea’s central bank cuts interest rates by a quarter percentage point to 3.5 in a bid to drive growth in the economy.</em>”<code class="language-plaintext highlighter-rouge">. The topic is:</code>”</td>
    </tr>
    <tr>
      <td style="text-align: center">Target output</td>
      <td style="text-align: center">Integer id corresponding to 1 of the 4 topic classes (“Sports”, “World”, “Business”, “Sci/Tech”)</td>
      <td style="text-align: center">(Tokenised) Text of label: “Sports” , “World”, “Business”, “Sci/Tech”</td>
    </tr>
    <tr>
      <td style="text-align: center">ML Model architecture</td>
      <td style="text-align: center">Pretrained model GPT2, followed by a multiclass classifier head. Only last layer trained</td>
      <td style="text-align: center">Pretrained model GPT2. All layers trainable for autoregressive output generation</td>
    </tr>
    <tr>
      <td style="text-align: center">Learning Objective</td>
      <td style="text-align: center">Minimise Cross entropy loss of classifying labels</td>
      <td style="text-align: center">Next token prediction objective of the Causal Autoregressive model</td>
    </tr>
    <tr>
      <td style="text-align: center">Inference</td>
      <td style="text-align: center">Argmax of a softmax over the output logits yields the predicted class id.</td>
      <td style="text-align: center">generate a sequence of upto 5 tokens that are most likely to follow the input prompt, and then use a verbaliser to map the classes to one of the 4 topic labels (or none)</td>
    </tr>
  </tbody>
</table>

<div class="row mt-3" style="width:600px; height:400px align:right">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sample_efficiency/data_points_vs_prompts_ag_news_w_bordered-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sample_efficiency/data_points_vs_prompts_ag_news_w_bordered-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sample_efficiency/data_points_vs_prompts_ag_news_w_bordered-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sample_efficiency/data_points_vs_prompts_ag_news_w_bordered.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<h4 id="experiment-observations-and-outcome">Experiment Observations and Outcome</h4>
<ul>
  <li>
<em>Seeing is believing</em>. The above experiment plot indicates that teaching a model through next-token prediction format 
helps the pretrained model learn quicker and with lesser data -&gt; more effective transfer learning!</li>
  <li>Steep gain in model performance with just a 100 prompts! When teaching a model through prompt-response formatted 
pairs we see that a model trained on a mere 100 instruction prompts reaches an accuracy of 50%, as compared to the more 
traditional classifier training approach which is at accuracy of 0.06%.</li>
  <li>The prompt-based learning (instruction tuning) method reaches an accuracy of over 60% when trained with 10,000 data 
points, in comparison to the mere 30% achieved by classifier-based model at the same 10,000 data point mark. If we 
linearly interpolate the training curves for model performance on training dataset size, then the classifier based 
model would have required 50,000 data points to achieve an accuracy of 60%.</li>
  <li>In conclusion: Similar to the results in the underlying paper [4], we find that in our primitive experiment, 
prompt-based instruction tuning of models was ~5x times more sample efficient than conventional classifier-head 
based models. In [4] the authors conduct more rigorous experiments where models are trained with better 
hyperparameters and to convergence, across multiple runs. They infer that ‘a prompt was equivalent to 100s of data 
points’ when evaluated on SUPERGLUE benchmark tasks.</li>
</ul>

<p><strong>Limitations of experiment</strong> (largely due to decisions of saving on training and infrastructure costs)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Limitation</th>
      <th style="text-align: center">Elaboration</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Under-training of models</td>
      <td style="text-align: center">I train each model (regardless of training dataset size) for only 5 epochs. This is well below convergence. Related works often train models for &gt; 50 epochs.[CITE]</td>
    </tr>
    <tr>
      <td style="text-align: center">Use of more primitive model GPT-2 and not powerful Causal LLM</td>
      <td style="text-align: center">The comparative gains of the prompt-based learning (instruction tuning) method is expected to be more stark when using a powerful FM like GPT-3, or the LLama models [CITE], and could have led to more decisive experiment results</td>
    </tr>
    <tr>
      <td style="text-align: center">Choice of rudimentary classification task</td>
      <td style="text-align: center">I chose the simplest task of sentence classification. It is not clear how my results extrapolate to more advanced tasks of say abstractive NLP. However, the simple classification setup was sufficient for purpose of my experiment.</td>
    </tr>
    <tr>
      <td style="text-align: center">Did not use a fewshot-prompting setup as baseline</td>
      <td style="text-align: center">I do not provide any comparison to the performance reachable by pure prompting and in-context learning (no training scenario).</td>
    </tr>
    <tr>
      <td style="text-align: center">Evaluation of generative models is hard, and I did not build a rigourous-enough evals</td>
      <td style="text-align: center">Mapping the output of <br>generative models to discriminative labels has been notorious in the community for lack of rigour. For instance, huggingface reports evaluation hurdles and holes while carrying out LLM evaluation on the benchmark task of MCQ-MMLU [7]. I used a verbaliser to map the output of the first 5 generated tokens from the fine-tuned LLM to map to one of the 4 class labels using a scoring function such as bert-score and rouge. These are not without limitations, see <a href="https://eugeneyan.com/writing/llm-patterns/?utm_source=convertkit&amp;utm_medium=email&amp;utm_campaign=How+to+Match+LLM+Patterns+to+Problems%20-%2011495339#more-about-evals" rel="external nofollow noopener" target="_blank">here</a>.</td>
    </tr>
  </tbody>
</table>

<h3 id="2-reflections"> Main reflections </h3>
<p>Here I focus on elementary realisations that grounded my understanding of what made prompting so powerful
(other than the fact that today’s FMs were trained on massive amounts of data) <d-footnote> This is summed up 
well by the quote "Transfer learning makes FMs possible, but it is the scale that makes them powerful 
INSERT reference" </d-footnote>).</p>
<ul>
  <li>
<code class="language-plaintext highlighter-rouge">Talk to and teach the model in its native tongue</code>: Prompting a model for completion in a certain task 
during inference is like interrogating the model in the native tongue it was taught in. By 
the same intuition, it would be more effective to continue fine-tune the model with a learning objective 
similar to what its pretraining objective (e.g. next token prediction, denoising) on datasets refomatted as 
pompt-completion pairs. This may be common knowledge now, but I still have a deep appreciation for this point.
    <ul>
      <li>I would <strong>strongly</strong> recommend this excellent review paper on the different LM architectures, training 
      objectives and their relationship with prompts to conduct various NLP tasks: [pretrain, prompt, predict]
(https://arxiv.org/abs/2107.13586)[4].</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">All tasks can be viewed as generation</code>: Almost all non-generative NLP tasks that were traditionally thought of as 
discriminative (e.g.Topic Classification, structured extraction (NER, EL, extractive QA), etc) could be posed as a 
generative task using prompt-completion frameworks. Instead of trying to modify the method (hammer)  to work with the task (bolt), transform the task to 
a nail instead. Some examples:
    <ul>
      <li>Extract structured events (NER + Relation extraction) from natural language: Authors design few-shot prompts in 
form of semi-instantiated Python classes for <code class="language-plaintext highlighter-rouge">Event</code>, with desired entities and relations as attributes of 
the class. They then prompt code-to-text FMs to complete the extraction of remainder 
attributes from the text</li>
      <li>Old gem, but I found myself revisiting the T5 paper which reverberates the philosophy of viewing 
all tasks as generation (sequence to sequence generation)</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">The same fine-tuning recipe behind most of the recent successful LLM apps</code> The recent surge of 
successful prompt-based technologies built atop general purpose FMs, whether chat-agents, coding assistants, or 
natural-language search tools, all inherit from the same 3-stage fine-tuning recipe as seen in Figure 2.</li>
</ul>

<div class="row mt-3" style="width:800px; height:500px align:right">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sample_efficiency/fm_to_chatgpts-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sample_efficiency/fm_to_chatgpts-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sample_efficiency/fm_to_chatgpts-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sample_efficiency/fm_to_chatgpts.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>The 3-stages of the fine-tuning recipe indicated above:
    <ul>
      <li>
<strong>Pretrained FM Selection</strong>: Take a foundational LLM which was pretrained at scale on 
subject domains and data formats that are relevant to your task. In case no suitable FM is available in 
domain of interest, one might also consider pretraining an LLM from scratch (e.g. BloomBergGPT)</li>
      <li>
<strong>Supervised Fine-tuning/Instruction Tuning</strong>: Adapt all or some of FM weights by supervised fine-tuning on
task-appropriate data points formatted as prompt-response pairs. In case of multi-task fine-tuning each task 
dataset may be formatted with its own distinct instruction sets or prefix(e.g. FLAN-lineage of 
models, T5). Parameter-Efficient Fine-Tuning (PEFT) methods may be used to update only part of the LLM weights 
[5].
        <ul>
          <li>In a broad strokes, ChatGPT is the output of instruction fine-tuning GPT-3.5+ over training examples that	
were in format of completing conversational dialog pairs (ChatGPT also involves another layer of 
fine-tuning where the model output is aligned to human preferences using RLHF)</li>
          <li>T5 was the result of taking the good old transformer architecture, pretraining it 
with a de-noising objective on unstructured text corpus, and then finally fine-tuning it on different 
tasks datasets that were converted into a format of text-to-text generation tasks with a task specific 
prefix. More recently, FLAN-T5 was released as a more powerful LLM after applying the instruction-tuning 
process to T5 at scale.</li>
          <li>Github CoPilot and OpenAI Codex utilise GPT-3 models that were continued to be trained on code 
repositories and then fine-tuned on language-to-code mapped datasets.</li>
        </ul>
      </li>
      <li>
<strong>(Optional) Reinforcement Learning from Human Feedback (RLHF)</strong> Align models to human preferences 
using RLHF. Ths involves another round of fine-tuning the models from step 2 with human feedback on preferred 
responses amongst multiple valid generated outputs. Such alignment is usually performed to tune the model 
to learn more abstract human notions such as safety, honesty, harmlessness, etc. A helpful overview is presented 
in <a href="https://huyenchip.com/2023/05/02/rlhf.html" rel="external nofollow noopener" target="_blank">Chip Huyen’s RLHF blogpost</a>[5].</li>
    </ul>
  </li>
  <li>Want to further fine-tune an LLM to your own downstream use-case with instruction-tuning, other PEFT approaches, 
or even RLHF? Well, you guessed correctly: prompt-response pairs are the labeled input-output data pairs you need to 
for your training routine again.</li>
</ul>

<h3 id="3-takeaway-remarks"> Prompt engineering vs fine-tuning </h3>

<p>A practical question faced is whether relying on in-context learning through carefully engineered prompts (without 
any training) is sufficient vs fine-tuning all or part of an LLM’s weights on a task.</p>

<div class="row mt-3" style="width:400px; height:300px align:right">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sample_efficiency/prompting_vs_fine_tuning-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sample_efficiency/prompting_vs_fine_tuning-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sample_efficiency/prompting_vs_fine_tuning-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/sample_efficiency/prompting_vs_fine_tuning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

    </div>
</div>

<p>There are a number of deciding factors in this situation are</p>
<ol>
  <li>Size of the foundation LLM: Literature demonstrates that the number of model parameters is correlated with 
improvements in prompting performance. In Figure 3 excerpted from one of the initial works in prompt tuning 
[Lester et al 2021][8], the authors show that while the advantages of fine-tuning over prompting are large for small 
model sizes (&lt;10B), this advantage is lost as model size approaches 10B. With these massive models prompting is 
sufficient for the LLM to understand the instructions and complete the task as per expectations, and engineering 
performance becomes comparable to that of fine-tuning and prompt-tuning.</li>
  <li>Does your task require specialised knowledge and complex reasoning? the more specific the knowledge required to 
perform the target task, it is useful if the LLM has been pretrained on knowledge (data and tasks) similar to the 
use-case domain. Prompting works well for simple tasks that rely on general-knowledge or are dependent on pattern 
recognition (e.g. parsing fixed structure from text). However, complex, multi-step reasoning or niche domain 
knowledge tasks may require fine-tuning for reliable and consistent performance. This is especially true if you 
are not using an LLM pretrained in the same domain as your task.</li>
  <li>Can you tune the instruction for desired output instead of tuning the model? There is a subset of PEFT methods 
such as prompt-tuning, prefix tuning, etc, that attach learnable parameters to the model input so as to maximise 
the probability of expected task output. This is in-between prompt engineering and fine-tuning of LLM weights and 
might benefit your use case.</li>
  <li>Do you have access to the actual model weights for fine-tuning? If you access the LLM behind an API, you cannot 
fine tune the model, prompt or prefix.</li>
  <li>What is economically and technically feasible for your application in production? Consider the production use 
case. Relying on the LLM’s in-context learning often requires very large descriptive prompts with advanced 
patterns like few-shot examples or Chain of Thought. Attaching a long prompt template for every inference data 
point can become very expensive in production. In such cases, if your task is very specific, then consider 
fine-tuning as this can bake in the prompt behaviour into the training examples, and thus improve the zero-shot 
model performance on the task. On the flipside, is it feasible to access and maintain the infrastructure required 
for self-hosting and fine-tuning an LLM?</li>
</ol>

<p><strong>Evaluation is your true north star:</strong> 
In my experience, the single most effective pattern to guide development of LLM-centric applications is evaluation.
Evaluate predictions from an LLM of your choice (e.g. ChatGPT, Llama, etc) on metrics that are relevant to your 
task are you able to reliably (reproducibly and consistently) get the performance you require from prompt 
engineering alone? Does this performance hold as you increase the size of the test set you evaluate on? If yes, then 
you in-context learning using prompting might be reasonable in your use case. If not, then you may consider 
approaches to adapt the LLM to your task.</p>

<p>There is no be a ‘golden goldilocks prompt’ that dramatically improves performance on your given task.
In [6] the authors insightfully experiment with the effect of prompt variability on the LLM task performance. They find that the gains of any one prompt usually disappear over multiple 
runs, and conclude that prompt size and format is not a dominant hyperparameter for the LLM. <d-footnote> Note, 
the tasks studied in the paper are simpler, short form NLP tasks. Long form abstractive reasoning tasks have been shown to benefit 
from more sophisticated prompts. A useful summary of the more advanced prompting techniques and where to use them 
can be found in [10]. As mentioned earlier, there are also PEFT methods that experiment with soft tunable prompts. 
A recent paper [`LLMs as Optimisers`](https://arxiv.org/abs/2309.03409)[11] uses meta-prompts to prompt the LLM to 
build its own task optimising prompt </d-footnote></p>

<h4 id="takeaways">Takeaways:</h4>
<ul>
  <li>If you want to use a smaller model for specific tasks and domains, the current industry/academia verdict is that 
fine-tuning can get you there faster and more reliably than prompt engineering.</li>
  <li>Directly adapting an LLM’s output to a task using fine-tuning with prompt-completion pairs (instruction tuning, or 
even RLHF which incorporates human preferences into the LLM tuning objective) is much more sample efficient than 
using conventional labeled input-output data pairs.</li>
  <li>Use PEFT methods to make fine-tuning LLMs more accessible and feasible. Methods such as LoRA (or QLoRA) are 
incredibly universal and can be applied to both instruction-tuning and RLHF.</li>
  <li>Build evals as a critical part of your workflow. An evaluation routine that captures the performance criteria that 
one most cares about is a critical aid to  deciding between approaches for LLM application.</li>
</ul>

<h3 id="acknowledgements">Acknowledgements</h3>
<p>I am very grateful to Corey Harper’s painstaking editorial review of the post, as well as Raahul Dutta for 
his helpful feedback.</p>

<h3 id="references">References</h3>

<p>[1] <a href="https://huyenchip.com/2023/04/11/llm-engineering.html" rel="external nofollow noopener" target="_blank">Building LLM applications for production</a> by Chip Huyen</p>

<p>[2] <a href="https://eugeneyan.com/writing/llm-patterns/" rel="external nofollow noopener" target="_blank">Patterns for Building LL-Based systems and products</a> by Eugene Yan</p>

<p>[3] <a href="https://wise-supervision.github.io/#Speakers" rel="external nofollow noopener" target="_blank">AKBC 2022 Keynote Talks</a></p>

<p>[4] Liu, Pengfei, et al. “Pre-train, prompt, and predict: A systematic survey of prompting methods in natural 
language processing.” ACM Computing Surveys 55.9 (2023): 1-35. <a href="https://dl.acm.org/doi/full/10.1145/3560815" rel="external nofollow noopener" target="_blank">[ACM]</a></p>

<p>[5] <a href="https://huyenchip.com/2023/05/02/rlhf.html" rel="external nofollow noopener" target="_blank">RLHF by Chip Huyen</a></p>

<p>[6] Scao, Teven Le, and Alexander M. Rush. “How many data points is a prompt worth?.” arXiv preprint arXiv:2103.08493 
(2021). <a href="https://arxiv.org/abs/2103.08493" rel="external nofollow noopener" target="_blank">[arXiv]</a></p>

<p>[7] <a href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" rel="external nofollow noopener" target="_blank">What’s going on with the Open LLM Leaderboard?</a></p>

<p>[8] Lester, Brian, Rami Al-Rfou, and Noah Constant. “The power of scale for parameter-efficient prompt tuning.” arXiv 
   preprint arXiv:2104.08691 (2021). <a href="https://arxiv.org/abs/2104.08691" rel="external nofollow noopener" target="_blank">[arXiv]</a></p>

<p>[9] GPT understands, too: Liu, X., et al. “GPT understands, too. arXiv.” arXiv preprint arXiv:2103.10385 (2021).</p>

<p>[10] <a href="https://towardsdatascience.com/advanced-prompt-engineering-f07f9e55fe01" rel="external nofollow noopener" target="_blank">Advanced Prompt-engineering</a> by 
Cameron R. Wolfe</p>

<p>[11] `Large Langugage Models as Optimisers: <a href="https://arxiv.org/abs/2309.03409" rel="external nofollow noopener" target="_blank">[arXiv]</a>
[12] Huggingface tutorial and implementation of Parameter Efficient Fine-Tuning methods: https://huggingface.
co/blog/peft</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Payal  Mitra. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
